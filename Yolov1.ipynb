{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "New Yolov1.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-S-31/CSE205-DS/blob/main/Yolov1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzYgLQahHNYX"
      },
      "source": [
        "Connecting to Google drive to upload dataset. This step is only required if you are using Google Colab and uploading dataset from Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y6KmakpRJu0",
        "outputId": "ad770cc2-120a-4b5a-c038-734dc4fa3afb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCivLdScKWHU"
      },
      "source": [
        "Importing all libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxCUs78JHTc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb04bb4b-1c57-4512-fe80-2c260441229d"
      },
      "source": [
        "! pip install comet_ml\n",
        "from comet_ml import Experiment\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from collections import OrderedDict\n",
        "from google.colab.patches import cv2_imshow\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage import io, transform\n",
        "from torchvision import transforms, datasets, utils\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting comet_ml\n",
            "  Downloading comet_ml-3.47.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting everett<3.2.0,>=1.0.1 (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n",
            "  Downloading everett-3.1.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (4.23.0)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (5.9.5)\n",
            "Collecting python-box<7.0.0 (from comet_ml)\n",
            "  Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n",
            "Collecting requests-toolbelt>=0.8.0 (from comet_ml)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.32.3)\n",
            "Collecting semantic-version>=2.8.0 (from comet_ml)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.17.0)\n",
            "Collecting simplejson (from comet_ml)\n",
            "  Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.2.3)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.16.0)\n",
            "Collecting wurlitzer>=1.0.2 (from comet_ml)\n",
            "  Downloading wurlitzer-3.1.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting dulwich!=0.20.33,>=0.20.6 (from comet_ml)\n",
            "  Downloading dulwich-0.22.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (13.9.3)\n",
            "Collecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n",
            "  Downloading configobj-5.0.9.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\n",
            "Downloading comet_ml-3.47.1-py3-none-any.whl (697 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m697.1/697.1 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dulwich-0.22.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (968 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m968.1/968.1 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\n",
            "Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\n",
            "Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: configobj\n",
            "  Building wheel for configobj (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configobj: filename=configobj-5.0.9-py2.py3-none-any.whl size=35615 sha256=86816600234816615f30c553ae45922e71820b96c25cb42fbe252e7a0eef300d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/6c/03/6c5e3cf1a6e4b9e2fc5c4409be4abc5a8268bd9c878739cb32\n",
            "Successfully built configobj\n",
            "Installing collected packages: everett, wurlitzer, simplejson, semantic-version, python-box, dulwich, configobj, requests-toolbelt, comet_ml\n",
            "  Attempting uninstall: python-box\n",
            "    Found existing installation: python-box 7.2.0\n",
            "    Uninstalling python-box-7.2.0:\n",
            "      Successfully uninstalled python-box-7.2.0\n",
            "Successfully installed comet_ml-3.47.1 configobj-5.0.9 dulwich-0.22.3 everett-3.1.0 python-box-6.1.0 requests-toolbelt-1.0.0 semantic-version-2.10.0 simplejson-3.19.3 wurlitzer-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHecGo64HXlm"
      },
      "source": [
        "We need to convert class ['Car','Cyclist'....] in the label file into an integer. As, we are not using label file into our model, we need not to use one hot encoding or other encoding techniques. We are simply converting it for ease of use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyzx2ROpIMcj"
      },
      "source": [
        "def class_encoding(label):\n",
        "\n",
        "  for i in range(label.shape[0]):\n",
        "\n",
        "    if label.iloc[i,0] == 'Car':\n",
        "      label.iloc[i,0] = 0\n",
        "    elif label.iloc[i,0] == 'Cyclist':\n",
        "      label.iloc[i,0] = 1\n",
        "    elif label.iloc[i,0] == 'Pedestrian':\n",
        "      label.iloc[i,0] = 2\n",
        "    elif label.iloc[i,0] == 'Tram':\n",
        "      label.iloc[i,0] = 3\n",
        "    elif label.iloc[i,0] == 'Truck':\n",
        "      label.iloc[i,0] = 4\n",
        "    elif label.iloc[i,0] == 'Van':\n",
        "      label.iloc[i,0] = 5\n",
        "    elif label.iloc[i, 0] == 'DontCare':\n",
        "      label.iloc[i, 0] = 6\n",
        "    elif label.iloc[i,0] == 'Misc':\n",
        "      label.iloc[i,0] = 7\n",
        "    elif label.iloc[i,0] == 'Person_sitting':\n",
        "      label.iloc[i,0] = 8\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaCrcuTHIWGC"
      },
      "source": [
        "Kitti Dataset has a different format for label file as compared to the YOLO format for label file. We need to convert format of our Kitti Dataset label file into format of YOLO label file.                                                       \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Note : We are rescaling coordinates of our bounding box into output image size and not the input image size as we need to compare the labels with the ouput of our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onG5FIbgIeYG"
      },
      "source": [
        "def transform_label(label, number_of_classes, image, input_image_size):\n",
        "\n",
        "  # In case of Bounding boxes, coordinate system doesnot start from bottom-left as we see normally in our mathematics, instead it starts from top-left corner\n",
        "  top_left_x = label[:,1]\n",
        "  top_left_y = label[:,2]\n",
        "  bottom_right_x = label[:,3]\n",
        "  bottom_right_y = label[:,4]\n",
        "\n",
        "  height = bottom_right_y - top_left_y\n",
        "  width = bottom_right_x - top_left_x\n",
        "  center_x = top_left_x + width/2\n",
        "  center_y = top_left_y + height/2\n",
        "\n",
        "  # Reducing the scale of the coordinates of bounding box in the label file into output image scale.\n",
        "  # We need to do this, so that at training and testing, we can compute loss easily, if all are in the same scale.\n",
        "  label[:,1] = (center_x/image.shape[1])*input_image_size\n",
        "  label[:,2] = (center_y/image.shape[0])*input_image_size\n",
        "  label[:,3] = (height /image.shape[0])*input_image_size\n",
        "  label[:,4] = (width/image.shape[1])*input_image_size\n",
        "\n",
        "  # Adding classes probabilites columns\n",
        "  target = np.zeros((label.shape[0],label.shape[1] + number_of_classes))\n",
        "  target[:,0:5] = label\n",
        "\n",
        "  for i in range(0,label.shape[0]):\n",
        "    if(target[i,0:1] == 0): # Prob_Class(Car) = 1 and rest 0, if Car is detected\n",
        "      target[i,5:6] = 1\n",
        "    elif(target[i,0:1] == 1): # Prob_Class(Cyclist) = 1 and rest 0, if Cyclist is detected\n",
        "      target[i,6:7] = 1\n",
        "    elif(target[i,0:1] == 2): # Prob_Class(Pedestrian) = 1 and rest 0, if Pedestrian is detected\n",
        "      target[i,7:8] = 1\n",
        "    elif(target[i,0:1] == 3): # Prob_Class(Tram) = 1 and rest 0, if Tram is detected\n",
        "      target[i,8:9] = 1\n",
        "    elif(target[i,0:1] == 4): # Prob_Class(Truck) = 1 and rest 0, if Truck is detected\n",
        "      target[i,9:10] = 1\n",
        "    elif(target[i,0:1] == 5): # Prob_Class(Van) = 1 and rest 0, if Van is detected\n",
        "      target[i,10:11] = 1\n",
        "\n",
        "  return target\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xShT0UeFkCyi"
      },
      "source": [
        "We need to preprocess the data. It means to keep the data i.e the input to our Convolutional Neural Network (CNN model) into an uniform form.\n",
        "Here, our input will be a dictionary of image and its label. Only, the images will be the input for our CNN model and label will be used for calculating loss. Largely, we only need to preprocess the data which we input in our CNN model with resizing, normalizing, mean subtraction etc.\n",
        "We also need to do padding on our label files, so that they become of same size tensor for collating in batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW-2inMnkFQd"
      },
      "source": [
        "class Resize(object):\n",
        "\n",
        "  def __init__(self, input_image_size):\n",
        "    # Input image size is the size of the image that we are putting it into our CNN Model. In this case, it is [270 X 270]\n",
        "    self.input_image_size = (input_image_size, input_image_size)\n",
        "\n",
        "  def __call__(self, data_sample):\n",
        "    image, label = data_sample['image'], data_sample['label']\n",
        "    image = transform.resize(image, self.input_image_size, preserve_range=True, anti_aliasing=True)\n",
        "\n",
        "    return {'image' : image, 'label' : label}\n",
        "\n",
        "class ToTensor(object):\n",
        "\n",
        "  def __call__(self, data_sample):\n",
        "    image, label = data_sample['image'], data_sample['label']\n",
        "    image = image.transpose((2, 0, 1)) # Converting the image form from (H X W X C) into (C X H X W)\n",
        "\n",
        "    # If we donot use float() at end, by default, torch.from_numpy() will convert our input of our CNN model into a Float64 type\n",
        "    # We have to convert our CNN model type also in Float64 i.e Double or else it will throw error.\n",
        "    # By default, CNN model type is Float16, so better to convert the input into Float16 type here only\n",
        "    return {'image' : torch.from_numpy(image).float(),\n",
        "            'label' : torch.from_numpy(label).float()\n",
        "            }\n",
        "\n",
        "class Normalization(object):\n",
        "\n",
        "  def __call__(self, data_sample):\n",
        "    image, label = data_sample['image'], data_sample['label']\n",
        "\n",
        "    image_mean = np.mean(image, axis = 0)\n",
        "    image_std = np.std(image, axis = 0)\n",
        "    image = (image_mean-image) / (image_std)\n",
        "\n",
        "    return {'image' : image, 'label' : label}\n",
        "\n",
        "# Our Dataset has different lengths data in our label file, so when stacking into a single batch during training, it throw error because of\n",
        "# variable dimensions. One of the solution is to pad the label file with an arbitrary number.\n",
        "class BatchPadding(object):\n",
        "\n",
        "  def __init__(self, pad):\n",
        "    self.pad = pad\n",
        "\n",
        "  def __call__(self, data_sample):\n",
        "    image, label = data_sample['image'], data_sample['label']\n",
        "    batched_label = np.zeros((self.pad,label.shape[1]))\n",
        "    batched_label[0:label.shape[0],:] = label\n",
        "\n",
        "    return {'image' : image, 'label' : batched_label}\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpTW24lZIr0w"
      },
      "source": [
        "Here, we are defining a class for our dataset. For our problem of Object Detection for Self Driving Cars, we are using KittiDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nlWbN5nIxZK"
      },
      "source": [
        "class KittiDataset(Dataset):\n",
        "\n",
        "    def __init__(self, labels_dir, images_dir, number_of_classes, input_image_size, transform=None):\n",
        "      self.labels_dir = labels_dir\n",
        "      self.images_dir = images_dir\n",
        "      self.number_of_classes = number_of_classes\n",
        "      self.input_image_size = input_image_size\n",
        "      self.transform = transform\n",
        "\n",
        "      self.labels_dict = {}\n",
        "      self.filename = []\n",
        "      self.__init__dataset()\n",
        "\n",
        "    def __init__dataset(self):\n",
        "      print('...............Initializing Dataset...............')\n",
        "\n",
        "      index = 0\n",
        "      for file in os.listdir(self.labels_dir):\n",
        "        print('Reading label file : ' + file + '...')\n",
        "\n",
        "        label_path = self.labels_dir + '/' + file\n",
        "        label = pd.read_csv(filepath_or_buffer=label_path, sep=' ', header=None, index_col=False)\n",
        "\n",
        "        # Taking out relevant features out from the label dataframe\n",
        "        label = label.iloc[:,[0,4,5,6,7]]\n",
        "        label.columns = ['Class','TopLeftX','TopLeftY','BottomRightX','BottomRightY']\n",
        "\n",
        "        # Class Encoding\n",
        "        # Car=0, Cyclist=1, Pedestrian=2, Tram=3, Truck=4, Van=5.......\n",
        "        class_encoding(label)\n",
        "\n",
        "        self.labels_dict[index] = label\n",
        "        self.filename.append(file[0:6])\n",
        "        index = index + 1\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels_dict)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      image_path = self.images_dir + '/' + self.filename[index] + '.png'\n",
        "      image = io.imread(image_path)\n",
        "\n",
        "      label = self.labels_dict[index]\n",
        "      label = label.to_numpy(dtype = np.float16)\n",
        "\n",
        "      # Convert the label into YOLO format (class, center_x, center_y, height, width, class_prob1 ..... class_probn)\n",
        "      target = transform_label(label, self.number_of_classes, image, self.input_image_size)\n",
        "\n",
        "      data_sample = {'image' : image, 'label' : target}\n",
        "\n",
        "      if self.transform:\n",
        "        data_sample = self.transform(data_sample)\n",
        "\n",
        "      return data_sample\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOvQc2zSI5QT"
      },
      "source": [
        "After creating the datatset class, we now need to create our CNN model class, where we define our resnet34 architecture and our own custom Functional Layer. We also going to unfrezze the resnet50 layers as these are already having predetermined weights for classifying objects and we don't want to flush off them in our back propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghm2SIYEI8TI"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, input_grids, number_of_cnn_output):\n",
        "\n",
        "      super(Net, self).__init__()\n",
        "      print('...............Initializing Convolutional Neural Network...............')\n",
        "      self.resnet50 = torchvision.models.resnet50(pretrained = True) # Using Resnet50 architecture\n",
        "\n",
        "      # Freezing all the layers\n",
        "      self.resnet50.layer1.requires_grad=False\n",
        "      self.resnet50.layer2.requires_grad=False\n",
        "      self.resnet50.layer3.requires_grad=False\n",
        "      self.resnet50.layer4.requires_grad=False\n",
        "\n",
        "      # Adding new Fully Connected and Sigmoid layer\n",
        "      self.number_of_filters = self.resnet50.fc.out_features\n",
        "      self.input_grids = input_grids\n",
        "      self.number_of_cnn_output = number_of_cnn_output\n",
        "\n",
        "      self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
        "      self.batch_norm_fc = nn.BatchNorm1d(num_features=self.number_of_filters)\n",
        "\n",
        "      self.fc1 = nn.Linear(self.number_of_filters, input_grids*number_of_cnn_output, bias=True)\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = self.resnet50.conv1(x)\n",
        "      x = self.resnet50.bn1(x)\n",
        "      x = self.resnet50.relu(x)\n",
        "      x = self.resnet50.maxpool(x)\n",
        "\n",
        "      x = self.resnet50.layer1(x)\n",
        "      x = self.resnet50.layer2(x)\n",
        "      x = self.resnet50.layer3(x)\n",
        "      x = self.resnet50.layer4(x)\n",
        "      x = self.resnet50.avgpool(x)\n",
        "\n",
        "      x = x.view(-1,self.resnet50.fc.in_features)\n",
        "      x = self.resnet50.fc(x)\n",
        "\n",
        "      x = self.leaky_relu(x)\n",
        "      x = self.batch_norm_fc(x)\n",
        "\n",
        "      x = self.fc1(x)\n",
        "      x = self.sigmoid(x)\n",
        "\n",
        "      return x\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHSSYHVuJDIK"
      },
      "source": [
        "After prediction, we will get many bounding boxes for a single class. To eliminate that, we need an algorithm to find which bounding box matches the ground truth bounding box by how much. We call this algorithm Intersection of Union (IOU).\n",
        "\n",
        "\n",
        "```\n",
        "IOU = (area of intersection) / (area of bounding box1 + area of bounding box2 - area of intersection)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x-gK3JJJGq7"
      },
      "source": [
        "def calculate_IOU(b1X, b1Y, b2X, b2Y, b3X, b3Y, b4X, b4Y):\n",
        "\n",
        "  # b1X, b1Y, b2X, b2Y corresponds to topleft and bottom right coordinates of bounding box1\n",
        "  # b3X, b3Y, b4X, b4Y corresponds to topleft and bottom right coordinates of bounding box2\n",
        "  xA = max(b1X,b3X)\n",
        "  yA = max(b1Y,b3Y)\n",
        "  xB = min(b2X,b4X)\n",
        "  yB = min(b2Y,b4Y)\n",
        "\n",
        "  area_intersection = max(0,xB-xA+1) * max(0,yB-yA+1)\n",
        "  area_of_boundingbox1 = (b2X-b1X+1) * (b2Y-b1Y+1)\n",
        "  area_of_boundingbox2 = (b4X-b3X+1) * (b4Y-b3Y+1)\n",
        "\n",
        "  iou = area_intersection/float(area_of_boundingbox1 + area_of_boundingbox2 - area_intersection + 0.0001)\n",
        "  return iou\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAsc1wGM8urT"
      },
      "source": [
        "After calculating IOU of all the bouding boxes, we need to return the bounding box whose IOU is the highest.\n",
        "\n",
        "\n",
        "---\n",
        "Remember the coordinates of the bounding box are scaled i.e x,y are offsets with respect to grid and h,w are scaled between 0 and 1 with respect to image height and width\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCijFzM48uBl"
      },
      "source": [
        "def find_highest_IOU(predicted_grid_output, ground_truth_grid_output, bounding_boxes, grid_cell, grid_offset, input_grids):\n",
        "  grid = np.sqrt(input_grids)\n",
        "  max_iou = 0\n",
        "  max_iou_index = 0\n",
        "\n",
        "  x_offset = int(grid_cell/grid)\n",
        "  y_offset = int(grid_cell%grid)\n",
        "\n",
        "  topLeftX = x_offset * grid_offset\n",
        "  topLeftY = y_offset * grid_offset\n",
        "\n",
        "  for number_of_bbox in range(0,bounding_boxes):\n",
        "\n",
        "    predicted_center_x = (predicted_grid_output[(number_of_bbox*5) + 1].item() * grid_offset) + topLeftX\n",
        "    predicted_center_y = (predicted_grid_output[(number_of_bbox*5) + 2].item() * grid_offset) + topLeftY\n",
        "    predicted_height = predicted_grid_output[(number_of_bbox*5) + 3].item() * grid_offset * grid\n",
        "    predicted_width = predicted_grid_output[(number_of_bbox*5) + 4].item() * grid_offset * grid\n",
        "\n",
        "    predicted_topLeftX = predicted_center_x - predicted_width/2\n",
        "    predicted_topLeftY = predicted_center_y - predicted_height/2\n",
        "    predicted_bottomRightX = predicted_center_x + predicted_width/2\n",
        "    predicted_bottomRightY = predicted_center_y + predicted_height/2\n",
        "\n",
        "    ground_truth_topLeftX = ground_truth_grid_output[1].item() - ground_truth_grid_output[4].item()/2\n",
        "    ground_truth_topLeftY = ground_truth_grid_output[2].item() - ground_truth_grid_output[3].item()/2\n",
        "    ground_truth_bottomRightX = ground_truth_grid_output[1].item() + ground_truth_grid_output[4].item()/2\n",
        "    ground_truth_bottomRightY = ground_truth_grid_output[2].item() + ground_truth_grid_output[3].item()/2\n",
        "\n",
        "    iou = calculate_IOU(predicted_topLeftX,predicted_topLeftY,predicted_bottomRightX,predicted_bottomRightY,\n",
        "                        ground_truth_topLeftX,ground_truth_topLeftY,ground_truth_bottomRightX,ground_truth_bottomRightY)\n",
        "\n",
        "    if(iou > max_iou):\n",
        "      max_iou = iou\n",
        "      max_iou_index = number_of_bbox\n",
        "\n",
        "  return max_iou_index\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDZTDH6dJWlD"
      },
      "source": [
        "Now, we will calculate the Loss function. It comprises of three losses :\n",
        "\n",
        "*   Classification Loss : if object is detected, the mean squared error loss of class probabilites\n",
        "*   Localization Loss : if object is detected, the mean squared error loss of coordinates of bounding box\n",
        "*   Confidence Loss : the mean squared error loss of box confidence, when object is detected and when it is not\n",
        "\n",
        "In the end, we will mulitply our loss with lambda_coord and lambda_noobject which regularize the imbalance and reduce the effect of background noise\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h866UlMIy9la"
      },
      "source": [
        "def yolo_loss(batched_output, batched_label, input_grids, grid_offset, bounding_boxes, classes, lambda_coord, lambda_noobject):\n",
        "\n",
        "  total_loss = torch.tensor([0], dtype=torch.float)\n",
        "  grid = np.sqrt(input_grids)\n",
        "\n",
        "  for batch_number in range(batched_output.size()[0]):\n",
        "\n",
        "    classification_loss = torch.tensor([0], dtype=torch.float)\n",
        "    localization_loss_centerpoint = torch.tensor([0], dtype=torch.float)\n",
        "    localization_loss_aspect_ratio = torch.tensor([0], dtype=torch.float)\n",
        "    confidence_loss_object = torch.tensor([0], dtype=torch.float)\n",
        "    confidence_loss_noobject = torch.tensor([0], dtype=torch.float)\n",
        "\n",
        "    for grid_cell in range(batched_output.size()[1]):\n",
        "\n",
        "      predicted_grid_output = batched_output[batch_number,grid_cell,:]\n",
        "\n",
        "      # Logic to get the center coordinates of grid cell\n",
        "      x_offset = int(grid_cell / grid)\n",
        "      y_offset = int(grid_cell % grid)\n",
        "\n",
        "      grid_cell_center_x = (x_offset*grid_offset) + (grid_offset/2)\n",
        "      grid_cell_center_y = (y_offset*grid_offset) + (grid_offset/2)\n",
        "\n",
        "      object_present = -1\n",
        "      ground_truth_grid_output = torch.Tensor()\n",
        "\n",
        "      for index in range(0,batched_label.size()[1]):\n",
        "        ground_truth_grid_output = batched_label[batch_number,index,:]\n",
        "        if (ground_truth_grid_output.sum() == 0):\n",
        "          break\n",
        "\n",
        "        ground_truth_center_x = ground_truth_grid_output[1].item()\n",
        "        ground_truth_center_y = ground_truth_grid_output[2].item()\n",
        "\n",
        "        object_class = ground_truth_grid_output[0].item() # Stores which object is present in the grid cell which is responsible for detecting\n",
        "\n",
        "        # Finding whether grid detects an object or not\n",
        "        if(object_class >= 0 and object_class < classes and ground_truth_center_x < (grid_cell_center_x+(grid_offset/2)) and ground_truth_center_x >= (grid_cell_center_x-(grid_offset/2))\n",
        "            and ground_truth_center_y < (grid_cell_center_y+(grid_offset/2)) and ground_truth_center_y >= (grid_cell_center_y-(grid_offset/2))):\n",
        "          object_present = object_class\n",
        "          break\n",
        "\n",
        "      # Calculating classification loss\n",
        "      if(object_present != -1):\n",
        "        partial_classification_loss = torch.tensor([0], dtype=torch.float)\n",
        "\n",
        "        for target_class in range(classes):\n",
        "          if(object_class != object_present):\n",
        "            partial_classification_loss = partial_classification_loss + (predicted_grid_output[5*bounding_boxes+target_class]) ** 2\n",
        "\n",
        "        classification_loss = classification_loss + partial_classification_loss + (1 - predicted_grid_output[5*bounding_boxes + int(object_present)])**2\n",
        "\n",
        "        # Calculating which bounding box has highest IOU with ground truth bounding box\n",
        "        highest_iou_bbox_index = find_highest_IOU(predicted_grid_output, ground_truth_grid_output, bounding_boxes, grid_cell, grid_offset, input_grids)\n",
        "\n",
        "        # Calculating localization loss of center points and aspect ratio\n",
        "\n",
        "        ground_truth_height = (ground_truth_grid_output[3])/(grid * grid_offset)\n",
        "        ground_truth_width = (ground_truth_grid_output[4])/(grid * grid_offset)\n",
        "        ground_truth_center_x = ((ground_truth_grid_output[1]) % grid_offset)/grid_offset\n",
        "        ground_truth_center_y = ((ground_truth_grid_output[2]) % grid_offset)/grid_offset\n",
        "\n",
        "        localization_loss_centerpoint = localization_loss_centerpoint + ((predicted_grid_output[1]-ground_truth_center_x))**2 + ((predicted_grid_output[2]-ground_truth_center_y))**2\n",
        "        localization_loss_aspect_ratio = localization_loss_aspect_ratio + (torch.sqrt(predicted_grid_output[3])-torch.sqrt(ground_truth_height))**2 + (torch.sqrt(predicted_grid_output[4])-torch.sqrt(ground_truth_width))**2\n",
        "\n",
        "        # Calculating Confidence loss, if object detected\n",
        "        confidence_loss_object = confidence_loss_object + (1 - predicted_grid_output[highest_iou_bbox_index*5])**2\n",
        "\n",
        "      # Calculating Confidence loss, if object not detected\n",
        "      else:\n",
        "        for number_of_bounding_box in range(bounding_boxes):\n",
        "          confidence_loss_noobject = confidence_loss_noobject + (predicted_grid_output[number_of_bounding_box*5])**2\n",
        "\n",
        "    total_loss = total_loss + classification_loss + lambda_coord*localization_loss_centerpoint + lambda_coord*localization_loss_aspect_ratio + confidence_loss_object + lambda_noobject*confidence_loss_noobject\n",
        "\n",
        "  batch_loss = total_loss/batched_output.size()[0]\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9vSE7fmJeRR"
      },
      "source": [
        "Our Convolutional Neural newtork is defined, dataset is defined, loss function is defined. Now, we will train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbFltpQ6JgdZ"
      },
      "source": [
        "def train(model, optimizer, scheduler, training_dataloader, training_batch_size, input_grids, grid_offset, number_of_cnn_output, bounding_boxes, classes, lambda_coord, lambda_noobject):\n",
        "\n",
        "  # This is inbuilt function of Pytorch and it is important to call it in training\n",
        "  # as few function like dropout and batch norm works differently in training mode than in evaluation mode\n",
        "  model.train()\n",
        "  batch_loss = 0\n",
        "\n",
        "  for batch_index, batched_sample in enumerate(training_dataloader):\n",
        "\n",
        "    batched_image = torch.tensor(batched_sample['image'], requires_grad=True, dtype=torch.float)\n",
        "    batched_label = torch.tensor(batched_sample['label'], requires_grad=True, dtype=torch.float)\n",
        "    batched_output = model(batched_image)\n",
        "    batched_output = batched_output.view(training_batch_size, input_grids, number_of_cnn_output) # Convert the output size into [N X GRIDS X (5 * B + C)]\n",
        "\n",
        "    loss = yolo_loss(batched_output, batched_label, input_grids, grid_offset, bounding_boxes, classes, lambda_coord, lambda_noobject)\n",
        "    print('Training Loss for batch_index : {} is {}'.format(batch_index,loss))\n",
        "    batch_loss = batch_loss + loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "  return batch_loss/len(training_dataloader)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q7NTe2l9Byp"
      },
      "source": [
        "Non Max Supression algorithm to remove duplicate bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYtmeEv5Q2PQ"
      },
      "source": [
        "def non_max_suppression(bbox_list, iou_threshold):\n",
        "\n",
        "  sorted_bbox_list = OrderedDict()\n",
        "  for key in sorted(bbox_list, reverse=True):\n",
        "    sorted_bbox_list[key] = bbox_list[key]\n",
        "\n",
        "  deleted_elements_list = []\n",
        "  for key1 in sorted_bbox_list:\n",
        "    for key2 in sorted_bbox_list:\n",
        "\n",
        "      if(key1 != key2):\n",
        "        bbox1 = sorted_bbox_list[key1]\n",
        "        bbox2 = sorted_bbox_list[key2]\n",
        "        iou = calculate_IOU(bbox1['tlx'], bbox1['tly'],bbox1['brx'],bbox1['bry'],bbox2['tlx'],bbox2['tly'],bbox2['brx'],bbox2['bry'])\n",
        "\n",
        "        if(iou >= iou_threshold):\n",
        "          if key2 not in deleted_elements_list:\n",
        "            deleted_elements_list.append(key2)\n",
        "\n",
        "  for del_ele in deleted_elements_list:\n",
        "    del sorted_bbox_list[del_ele]\n",
        "\n",
        "  return sorted_bbox_list\n",
        ""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxOqC1ERJ5xp"
      },
      "source": [
        "We have trained our model, now we will validate our model. Validation is required to tune our model parameters and hyper parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA1zdHgdJ8Fp"
      },
      "source": [
        "def validation(model, validation_dataloader, validation_batch_size, classes, input_grids, grid_offset, number_of_cnn_output, saving_results_path, bounding_boxes, validation_images_dir, lambda_coord, lambda_noobject, object_detected_threshold, box_confidence_threshold, iou_threshold):\n",
        "\n",
        "  # This is inbuilt function of Pytorch and it is important to call it in training\n",
        "  # as few function like dropout and batch norm works differently in training mode than in evaluation mode\n",
        "\n",
        "  model.eval()\n",
        "  batch_loss = 0\n",
        "  filename=[]\n",
        "  image_index = 0\n",
        "  grid = np.sqrt(input_grids)\n",
        "\n",
        "  for file in os.listdir(validation_images_dir):\n",
        "    filename.append(file[0:6])\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_index, batched_validation_sample in enumerate(validation_dataloader):\n",
        "      validation_image = batched_validation_sample['image']\n",
        "      validation_label = batched_validation_sample['label']\n",
        "      output = model(validation_image)\n",
        "      output = output.view(validation_batch_size, input_grids, number_of_cnn_output)\n",
        "\n",
        "      image_path = validation_images_dir + '/' + filename[image_index] + '.png'\n",
        "      original_image = io.imread(image_path)\n",
        "\n",
        "      loss = yolo_loss(output, validation_label, input_grids, grid_offset, bounding_boxes, classes, lambda_coord, lambda_noobject)\n",
        "      print('Validation Loss for batch_index : {} is {}'.format(batch_index,loss))\n",
        "      batch_loss += loss.item()\n",
        "\n",
        "      for batch_number in range(validation_batch_size):\n",
        "        bbox_list = OrderedDict()\n",
        "        for grid_cell in range(input_grids):\n",
        "          grid_output = output[batch_number,grid_cell,:]\n",
        "\n",
        "          # Logic to get the center, top-left and bottom-right coordinates of grid cell\n",
        "          x_offset = int(grid_cell / grid)\n",
        "          y_offset = int(grid_cell % grid)\n",
        "\n",
        "          grid_cell_topleftX = x_offset * grid_offset\n",
        "          grid_cell_topleftY = y_offset * grid_offset\n",
        "\n",
        "          for number_of_bounding_box in range(bounding_boxes):\n",
        "            if(grid_output[number_of_bounding_box*5] > object_detected_threshold):\n",
        "\n",
        "              class_probabilities = grid_output[5*bounding_boxes:]\n",
        "              max_class_probabilites, index = torch.max(class_probabilities,0)\n",
        "\n",
        "              center_x = grid_output[(number_of_bounding_box*5)+1] * grid_offset + grid_cell_topleftX\n",
        "              center_y = grid_output[(number_of_bounding_box*5)+2] * grid_offset + grid_cell_topleftY\n",
        "              height = grid_output[(number_of_bounding_box*5)+3] * grid * grid_offset\n",
        "              width = grid_output[(number_of_bounding_box*5)+4] * grid * grid_offset\n",
        "\n",
        "              top_left_x = int((((center_x - (width/2))/(grid*grid_offset))*original_image.shape[1]).item())\n",
        "              top_left_y = int((((center_y - (height/2))/(grid*grid_offset))*original_image.shape[0]).item())\n",
        "              bottom_right_x = int((((center_x + (width/2))/(grid*grid_offset))*original_image.shape[1]).item())\n",
        "              bottom_right_y = int((((center_y + (height/2))/(grid*grid_offset))*original_image.shape[0]).item())\n",
        "\n",
        "              predicted_strength = max_class_probabilites.item() * grid_output[number_of_bounding_box*5].item()\n",
        "              accuracy = round(predicted_strength * 100,2)\n",
        "\n",
        "              if(top_left_x >= 0 and top_left_y >= 0 and bottom_right_x >= 0 and bottom_right_y >= 0 and predicted_strength >= box_confidence_threshold):\n",
        "                bbox_list[predicted_strength]={'tlx':top_left_x,'tly':top_left_y,'brx':bottom_right_x,'bry':bottom_right_y,'accuracy':accuracy,'index':index.item()}\n",
        "\n",
        "        updated_bbox_list = non_max_suppression(bbox_list,iou_threshold)\n",
        "        name = 'Unknown'\n",
        "        color = (130,130,130)\n",
        "\n",
        "        for key in updated_bbox_list:\n",
        "          bbox = updated_bbox_list[key]\n",
        "          object_class = bbox['index']\n",
        "\n",
        "          top_left_x = bbox['tlx']\n",
        "          top_left_y = bbox['tly']\n",
        "          bottom_right_x = bbox['brx']\n",
        "          bottom_right_y = bbox['bry']\n",
        "\n",
        "          if(object_class == 0):\n",
        "            name = 'Car'\n",
        "            color = (255,255,255)\n",
        "          elif(object_class == 1):\n",
        "            name = 'Cyclist'\n",
        "            color = (0,0,255)\n",
        "          elif(object_class == 2):\n",
        "            name = 'Pedestrian'\n",
        "            color = (0,255,0)\n",
        "          elif(object_class == 3):\n",
        "            name = 'Tram'\n",
        "            color = (255,0,0)\n",
        "          elif(object_class == 4):\n",
        "            name = 'Truck'\n",
        "            color = (0,255,255)\n",
        "          elif(object_class == 5):\n",
        "            name = 'Van'\n",
        "            color = (255,0,255)\n",
        "\n",
        "          name = name + '(' + str(bbox['accuracy']) + ')'\n",
        "\n",
        "          original_image = cv2.rectangle(original_image,(top_left_x,top_left_y),(bottom_right_x,bottom_right_y),color,2)\n",
        "          original_image = cv2.rectangle(original_image,(top_left_x,top_left_y-30),(top_left_x+125,top_left_y),color,cv2.FILLED)\n",
        "          cv2.putText(original_image, name, (top_left_x, top_left_y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1)\n",
        "\n",
        "        save_image_path = saving_results_path + str(batch_index) + '.png'\n",
        "        file_saved = cv2.imwrite(save_image_path, original_image)\n",
        "\n",
        "      image_index +=1\n",
        "\n",
        "  return batch_loss/len(validation_dataloader)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU9gLfZyJox3"
      },
      "source": [
        "We will finish our program by writing a main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nkCwlPQJqWr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "outputId": "ad8a2318-b58d-40d9-818f-c2dce8834023"
      },
      "source": [
        "def main():\n",
        "    print('...............Main Function starts...............')\n",
        "\n",
        "    # Training Settings\n",
        "    base_lr = 0.000001 # Hyper parameters\n",
        "    max_lr = 0.001 # Hyper parameters\n",
        "    momentum = 0.9 # Hyper parameters\n",
        "    epochs = 100  # Hyper parameters\n",
        "    training_batch_size = 2 # Hyper parameters\n",
        "    validation_batch_size = 1\n",
        "\n",
        "    object_detected_threshold = 0.25 # Model Parameters\n",
        "    box_confidence_threshold = 0.5 # Model Parameters\n",
        "    iou_threshold = 0.4 # Model Parameters\n",
        "\n",
        "    input_image_size = 224 # Model Parameters\n",
        "    input_grids = 7*7 # Model Parameters\n",
        "    grid_offset = input_image_size/np.sqrt(input_grids) # Model Parameters\n",
        "    bounding_boxes = 2 # Model Parameters\n",
        "\n",
        "    classes = 6 # Model Parameters\n",
        "    number_of_cnn_output = (5*bounding_boxes) + classes # Model Parameters\n",
        "    lambda_coord = 5 # Model Parameters\n",
        "    lambda_noobject = 0.5 # Model Parameters\n",
        "\n",
        "    save_model = False\n",
        "    seed = 1\n",
        "    logging = True\n",
        "    steps_completed = 0\n",
        "    last_epoch_loss = 0\n",
        "    number_of_training_data = 0\n",
        "    training_loss = 0\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Comet ML Settings for visualizing loss function and hyper parameters\n",
        "    if(logging):\n",
        "      experiment = Experiment(api_key=\"Vxlozksi1tLwXJlmZYjfQVm7w\", project_name=\"object-detection\", workspace=\"jayfartiyal\")\n",
        "      hyper_parameters = {\"lr\": base_lr, \"epochs\": epochs, \"batch_size\":training_batch_size}\n",
        "      experiment.log_parameters(hyper_parameters)\n",
        "\n",
        "    # Images and labels Directory\n",
        "    training_labels_dir = r'/content/gdrive/My Drive/kitti_single_nano/training/label_2'\n",
        "    training_images_dir = r'/content/gdrive/My Drive/kitti_single_nano/training/image_2'\n",
        "    validation_images_dir = r'/content/gdrive/My Drive/kitti_single_nano/validation/image_2'\n",
        "    validation_labels_dir = r'/content/gdrive/My Drive/kitti_single_nano/validation/label_2'\n",
        "    saving_model_path = r'/content/gdrive/My Drive/kitti_single_nano/validation/resnet50_class6_sgd_clr_version1.cnn.pt'\n",
        "    saving_results_path = r'/content/gdrive/My Drive/kitti_single_nano/validation/results/image'\n",
        "\n",
        "    if(save_model == False):\n",
        "      #Inititalizing model and optimizer\n",
        "      model = Net(input_grids, number_of_cnn_output)\n",
        "      optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=momentum)\n",
        "      scheduler = optim.lr_scheduler.CyclicLR(optimizer=optimizer,base_lr=base_lr,max_lr=max_lr,step_size_up=250)\n",
        "\n",
        "      print('...............Convolutional Neural Network model and optimizer has been initialized...............')\n",
        "\n",
        "      # Retrieving model and optimizer states if present\n",
        "      if(os.path.isfile(saving_model_path)):\n",
        "        print('Previous Model state found...............')\n",
        "\n",
        "        checkpoint = torch.load(saving_model_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        steps_completed = checkpoint['steps_completed']\n",
        "        last_epoch_loss = checkpoint['last_epoch_loss']\n",
        "\n",
        "        print('Previous Model and optimizer states has been retrieved...............')\n",
        "        print('{} steps completed..........'.format(steps_completed))\n",
        "        print('Last epoch cycle loss : {}..........'.format(last_epoch_loss))\n",
        "      else:\n",
        "        print('Previous Model state not found !!!...............')\n",
        "\n",
        "      save_model = True # After the finish of the program, it should save the model\n",
        "\n",
        "    # Creating transform to apply on training dataset\n",
        "    training_dataset_transform = transforms.Compose([\n",
        "                                         BatchPadding(100),\n",
        "                                         Resize(input_image_size),\n",
        "                                         Normalization(),\n",
        "                                         ToTensor()])\n",
        "\n",
        "    # Creating transform to apply on validation dataset\n",
        "    validation_dataset_transform = transforms.Compose([\n",
        "                                         BatchPadding(100),\n",
        "                                         Resize(input_image_size),\n",
        "                                         Normalization(),\n",
        "                                         ToTensor()])\n",
        "\n",
        "    # Creating training and validation dataset instance\n",
        "    training_dataset = KittiDataset(labels_dir=training_labels_dir, images_dir=training_images_dir, number_of_classes=classes, input_image_size=input_image_size, transform=training_dataset_transform)\n",
        "    validation_dataset = KittiDataset(labels_dir=validation_labels_dir, images_dir=validation_images_dir, number_of_classes=classes, input_image_size=input_image_size, transform=validation_dataset_transform)\n",
        "    number_of_training_data = training_dataset.__len__()\n",
        "\n",
        "    training_dataloader = DataLoader(dataset=training_dataset, batch_size=training_batch_size, shuffle=True, drop_last=True)\n",
        "    validation_dataloader = DataLoader(dataset=validation_dataset, batch_size=validation_batch_size)\n",
        "\n",
        "    print('...............Training and Validation Dataloader initialized...............')\n",
        "    print('...............Training is starting...............')\n",
        "\n",
        "    for epoch in range(0,epochs):\n",
        "      training_loss = train(model, optimizer, scheduler, training_dataloader, training_batch_size, input_grids, grid_offset, number_of_cnn_output, bounding_boxes, classes, lambda_coord, lambda_noobject)\n",
        "      print('Training Loss for epoch :{} is {}'.format(epoch,training_loss))\n",
        "\n",
        "      # Logging training loss for hyper parameter tuning\n",
        "      if(logging):\n",
        "        experiment.log_metric(\"Training Loss\", training_loss)\n",
        "\n",
        "      print('................Validation is starting.................')\n",
        "      validation_loss = validation(model, validation_dataloader, validation_batch_size, classes, input_grids, grid_offset, number_of_cnn_output, saving_results_path, bounding_boxes, validation_images_dir, lambda_coord, lambda_noobject, object_detected_threshold, box_confidence_threshold, iou_threshold)\n",
        "      print('Validation Loss for epoch :{} is {}'.format(epoch,validation_loss))\n",
        "\n",
        "      # Logging training loss for hyper parameter tuning\n",
        "      if(logging):\n",
        "        experiment.log_metric(\"Validation Loss\", validation_loss)\n",
        "\n",
        "    steps_completed += int(((number_of_training_data)/training_batch_size)*epochs)\n",
        "\n",
        "    if (save_model):\n",
        "        torch.save({\n",
        "          'steps_completed': steps_completed,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'last_epoch_loss' : training_loss}, saving_model_path)\n",
        "        print('..............Convolutional Neural Network Model parameters are saved...............')\n",
        "\n",
        "\n",
        "    print('...............Model is now trained over : {} steps...............'.format(steps_completed))\n",
        "    print('...............Last epoch cycle loss : {} ...............'.format(last_epoch_loss))\n",
        "    print('...............Current cycle last epoch loss : {} ...............'.format(training_loss))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...............Main Function starts...............\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/jayfartiyal/object-detection/c64c92b22f9148d385502dc4a17cdba1\n",
            "\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/content' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...............Initializing Convolutional Neural Network...............\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 148MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...............Convolutional Neural Network model and optimizer has been initialized...............\n",
            "Previous Model state not found !!!...............\n",
            "...............Initializing Dataset...............\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/gdrive/My Drive/kitti_single_nano/training/label_2'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e742756c89c8>\u001b[0m in \u001b[0;36m<cell line: 132>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-e742756c89c8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# Creating training and validation dataset instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mtraining_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKittiDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_labels_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_images_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_image_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_image_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_dataset_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mvalidation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKittiDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_labels_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_images_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_image_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_image_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_dataset_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mnumber_of_training_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-9e09950f5b7c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, labels_dir, images_dir, number_of_classes, input_image_size, transform)\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-9e09950f5b7c>\u001b[0m in \u001b[0;36m__init__dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reading label file : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/kitti_single_nano/training/label_2'"
          ]
        }
      ]
    }
  ]
}